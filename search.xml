<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[IT人都应该了解的存储知识]]></title>
    <url>%2F2020%2F03%2F02%2FIT%E4%BA%BA%E9%83%BD%E5%BA%94%E8%AF%A5%E4%BA%86%E8%A7%A3%E7%9A%84%E5%AD%98%E5%82%A8%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[存储作为一个非常普遍、非常重要的IT领域，因为其复杂性，可能把很多人拒之门外。所以让非存储领域的IT人都能懂存储是一种需求。本文大致按照历史的脉络来介绍存储系统的发展，希望对大家有帮助。 从一个硬盘说起从大家接触的第一台台式机或者笔记本电脑开始，就在自己的硬盘上存储信息了，一个硬盘就是我们最早接触的“存储系统”。但是只是硬盘不够靠谱，因为我们知道硬盘损坏也是常有的事。即便现在的硬盘技术也是能够在应对某个局部区域损坏的情况下做出修复，但是无法恢复的故障还是是有发生。怎么在单个硬盘可能存在故障的情况下，仍然保证数据完整呢？ RAID技术诞生了为了解决在一个盘故障下的数据保护问题，RAID技术诞生。从1987年刚开始出现到现在RAID技术已经发展了30多年，刚开始的RAID1相当于多写一份，可以允许其中一块盘故障，RAID5利用校验机制降低了RAID1的数据存储成本问题，等等。从RAID的技术发展上我们就可以看出用户对存储系统的几个需求：持久性高、可用性高、性能高、成本低等。但是RAID技术只能解决单机的数据保护，如果一台机器坏了，那这台机器的RAID组可能也会丢失，或者只是一段时间无法访问，但这也足够使我们烦恼。这里我们看到，存储系统地设计初衷，不仅仅面临数据保护问题，而且面临随时需要的数据的读写访问问题。 单机内冗余机制单机继续演化，从以前的单处理器，单内存条，单网卡，单磁盘阵列，单电源等，到所有组件都有一到多份冗余，即使单个组件，单边访问链路损坏，也不会有数据丢失和数据无法访问的问题。这就是集中式的传统存储系统。从某种程度上说，我们可能满足了所有的存储需求，或者说是满足了某个时代大家对存储系统的需求，毕竟集中式传统存储也已经发展了几十年时间。但是后来互联网、物联网发展起来，数据量呈现爆发式增长，传统集中式存储虽然以其高可靠性、高性能著称，但是其应对超大数据量如何快速扩缩成了问题，而且超大数据量带来的存储成本问题变得非常重要。另外一点，再可靠的单机式存储，在面对机柜、交换机、甚至机房的故障时，也是无能为力的。 分布式存储快速扩缩容问题、成本问题、更大的故障容忍需求，促使分布式存储出现，并得到快速发展。分布式存储相比传统存储，消除了厂商锁定，更多地依赖于软件，其硬件只是使用通用服务器。正是因为其不用依赖专用硬件，也没有中心节点，其扩容缩容能力很强。按需扩缩容满足了业务需求，也是降低了存储使用成本，以往的硬件lisences成本也省掉了。分布式集群中都是体积较小的通用服务器，可以很容易进行跨机柜、跨交换机、甚至跨机房组建集群，以实现很高的故障容忍能力。当然在满足更多需求的同事，分布式存储也引入了更多需要解决的问题。跨节点的冗余机制和成本问题、节点之间数据量和读写负载均衡问题、扩缩容对线上负载的影响问题，这些都是需要在分布式存储软件上需要解决的。]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb常用命令备忘]]></title>
    <url>%2F2018%2F08%2F28%2Fgdb%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[1、 暂停方式断点 breakpoint / b观察点 watchpoint捕捉点 catchpoint信号 signals线程停止 thread stops恢复程序 continue / c 2、帮助help 命令类别help breakpoint / status / files -&gt;具体命令的帮助 3、带上调试信息加-g例如：gcc -g hello.c -o hello 4、常用指令gdb listbreak 5break funcinfo breakrunn / nextc / continuep i / printbt 查看函数调用栈finish 退出函数q 退出gdb]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leveldb学习总结]]></title>
    <url>%2F2018%2F08%2F02%2Fleveldb%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这段时间在学习 LevelDB 这个经典的 KV 存储系统，先扔一张自己总结的图，后面有时间主机补充细节。]]></content>
      <categories>
        <category>存储</category>
      </categories>
      <tags>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式相关学习资料]]></title>
    <url>%2F2018%2F07%2F28%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[最近在网上搜到一些分布式相关的很好的资料，在这里汇总，方便学习和查找。以后不断更新~~ #Part-1 分布式存储相关分布式存储必读论文，作为一个必读论文清单记录。其中的部分论文我做了下载和整理。如下：[1] The Google File System. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung[2] Bigtable: A Distributed Storage System for Structured Data. Fay Chang, Jeffrey Dean, Sanjay Ghemawat, et[3] Spanner: Google’s Globally-Distributed Database. James C. Corbett, Jeffrey Dean, et[4] PacificA: Replication in Log-Based Distributed Storage Systems. Wei Lin, Mao Yang, et[5] Object Storage on CRAQ, High-throughput chain replication for read-mostly workloads. Jeff Terrace and Michael J. Freedman[6] Ceph: Reliable, Scalable, and High-Performance Distributed Storage. Sage A. Weil[7] Finding a needle in Haystack: Facebook’s photo storage. Doug Beaver, Sanjeev Kumar, Harry C. Li, Jason Sobel, Peter Vajgel[8] Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency. Brad Calder, Ju Wang, Aaron Ogus, Niranjan Nilakantan, et[9] The Chubby lock service for loosely-coupled distributed systems. Mike Burrows[10] Paxos Made Live – An Engineering Perspective. Tushar Chandra, Robert Griesemer，Joshua Redstone[11] Dynamo: Amazon’s Highly Available Key-Value Store。 Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从虚拟机中的文件到RADOS的object]]></title>
    <url>%2F2018%2F07%2F25%2F%E4%BB%8E%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E5%88%B0RADOS%E7%9A%84object%2F</url>
    <content type="text"><![CDATA[#从虚拟机中的文件到RADOS的objectCeph将虚拟机的卷映射到最终的OSD，经过了两次映射。第一次，从object名到PG做简单hash，并结合pool_id作为前缀，第二次，从PG到OSD通过CRUSH算法计算得到。我们想知道卷最终被映射到哪些主机、哪些OSD、哪些object还是比较麻烦的。我这里摸索出了，从虚拟机的某个文件找到该文件所在rados的object, osd, host的方法 (基于J版Ceph) 。如下： 查看文件的磁盘设备偏移地址12345[root@vmhost]# filefrag -v testFilesystem type is: 58465342File size of test is 35052 (9 blocks of 4096 bytes)ext: logical_offset: physical_offset: length: expected: flags:0: 0.. 8: 268435473.. 268435481: 9: eof 根据physical_offset的起始地址 268435473 计算所在的object编号（磁盘设备是顺序切分成object的）268435473/1024 = 262144.0166015625即此从第262144个object开始的,object名字是由prefix和后缀组成的，其中后缀是16进制的，262144转化成16进制为40000。 查看object的名字12345678[root@server]# rbd info efs-nfsd-test/quotatest18.imgrbd image 'quotatest18.img':size 8192 GB in 2097152 objectsorder 22 (4096 kB objects)block_name_prefix: rbd_data.4d79a4238e1f29format: 2features: layering, exclusive-lockflags: 找到对应的object prefix为: rbd_data.4d79a4238e1f29 通过下面这个命令找到该卷对应的所有的object：1rados -p efs-nfsd-test ls |grep 'rbd_data.4d79a4238e1f29' &gt;&gt; efs-nfsd-test-objects.txt 在这里面可以找到我们需要的object：rbd_data.4d79a4238e1f29.0000000000040000 找到对应的object映射到的osd123[root@server]# ceph osd map efs-nfsd-test rbd_data.4d79a4238e1f29.0000000000040000osdmap e188415 pool 'efs-nfsd-test' (16) object 'rbd_data.4d79a4238e1f29.0000000000040000' -&gt; pg 16.4fa43842 (16.42) -&gt; up ([0,20,13], p0) acting ([0,20,13], p0) 从osd找到对应的主机1[root@server]# ceph osd tree 如果需要可以对object的内容进行验证，导出对应object的内容1rados -p efs-nfsd-test get rbd_data.4d79a4238e1f29.0000000000040000 rbd_data.4d79a4238e1f29.0000000000040000.txt 用dd可以定位到对应位置的文件内容1234[root@vmhost]# dd if=rbd_data.4d79a4238e1f29.0000000000040000.txt of=rbd_data.4d79a4238e1f29.0000000000040000_2.txt bs=4k count=9 skip=179+0 records in9+0 records out36864 bytes (37 kB) copied, 0.000475126 s, 77.6 MB/s 其中skip的数目是以4k为单位的，可以从开始的filefrag -v test的数字计算出来268435473-262144*1024=17可以看到从object导出的文件内容和实际的文件内容是一致的。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git入门之同步上游代码]]></title>
    <url>%2F2018%2F07%2F24%2FGit%E5%85%A5%E9%97%A8%E4%B9%8B%E5%90%8C%E6%AD%A5%E4%B8%8A%E6%B8%B8%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[#Git 入门之同步上游代码作为开源社区的贡献者，我们经常需要将自己的Github仓库从社区同步。这里从网上查，加上自己实践总结一下方法。 ##同步上游的代码到自己的仓库，并提交到自己的Github仓库 添加一个upstream仓库 git remote add upstream &lt;原作者项目的URL&gt; 查看已经添加的上游仓库 git remote -v 这里比如说添加ceph的上游社区仓库： git remote add ceph_upstream https://github.com/ceph/ceph.git 将上游的项目更新到本地 git fetch ceph_upstream 选择本地的master分支 git checkout master 将本地的分支与ceph_upstream的master分支合并 git merge ceph_upstream/master 将本地的内容提交到自己的Github仓库 git remote -vgit push git@github.com:&lt;Github用户&gt;/ceph.git 参考：https://blog.csdn.net/sjt19910311/article/details/50596714]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell学习]]></title>
    <url>%2F2018%2F06%2F22%2Fshell%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[#Shell 学习因为平时写 Shell 脚本比较多，并且 Shell 的语法规则比较零散，这里做一些学习和实践总结。 ##Shell 字典字典的声明1declare -A all_service 字典的赋值1all_service=([key1]="value1" [key2]="value2" [key3]="vlaue3") 字典的取值 取 value 值如取 key1 对应的 value1：1$&#123;all_service[key1]&#125; 如取所有的 value：1$&#123;all_service[*]&#125; 取key值如取所有的 key：1$&#123;!all_service[*]&#125; 示例123456789101112131415161718#!/bin/bashdeclare -A all_serviceall_service=([ntpd]="on" [lldpad]="on" [firewalld]="off")echo "---Get value1---"echo $&#123;all_service[ntpd]&#125;echo "---Get all values---"echo $&#123;all_service[*]&#125;echo "---Get all keys---"echo $&#123;!all_service[*]&#125;echo "---Get all key:value pairs---"for services in $(echo $&#123;!all_service[*]&#125;);do echo "$&#123;services&#125; : $&#123;all_service[$&#123;services&#125;]&#125;"done 运行结果1234567891011&gt;sh test.sh---Get value1---on---Get all values---on on off---Get all keys---lldpad ntpd firewalld---Get all key:value pairs---lldpad : onntpd : onfirewalld : off 参考[1] https://blog.csdn.net/jeremy_yangt/article/details/49100773[2] https://blog.csdn.net/u014297722/article/details/54601660?utm_source=itdadao&amp;utm_medium=referral]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SRE]]></title>
    <url>%2F2018%2F06%2F10%2FSRE%2F</url>
    <content type="text"><![CDATA[从 2016 年底就开始阅读 《SRE Google 运维解密》 这本书，断断续续看得差不多。对于 Google 这种超大规模的互联网公司，他的运维体系在相当长的时间内都是领先业界很多，虽然说其公开的资料已经是 Google 几年前的事情了，但是其体系还是有很大的研究和实践价值。这本书的重点是指导思想+具体实践，其中的关键词是分布式。众所周知，Google 在分布式系统方面的理论和实践相当先进，其分布式存储 GFS、分布式计算 MapReduce，分布式数据库 BigTable 也是被广泛地研究与学习。 现在开始进入正文，什么是 SRE ？SRE 的全称是 Site Reliability Engineering，直译是“站点可靠性工程师”。他承担运维工作，但不是传统意义上的运维工程师。SRE 的要务：（1） 确保长期参与研发工作（2） 在保障服务 SLO 的前提下最大化迭代速度（3） 监控系统（4） 应急事件处理（5） 变更管理（6） 需求预测和容量规划（7） 资源部署（8） 效率与性能 #指导思想SRE 其实是 DevOps 的一种具体实践。 图一 Dev，致力于快速发布新特性；Ops，致力于生产系统的稳定可用。两者结合解决了开发和运维之间的矛盾，并且统一了团队的目标，即：在保障服务 SLO （服务等级目标） 的前提下最大化迭代速度。下表描述了 DevOps 五大支柱和相对应的 SRE 实践： DevOps SRE 减少组织内的孤立 通过在整个技术栈使用同样的工具和技术来与开发者一同承担 接受失效是常态 使用一个公式在新版本发布和事故/失效之间做平衡 实现渐进式变更 实现渐进式变更 使用工具和自动化 鼓励“将今年的工作自动化掉”，最少化人工操作，以此将精力集中到能给系统带来长期价值的工作上 测量一切 坚信运维是一个软件问题。定义标准化的方法来测量可用性，运行时间，中断和苦力 表一 1 风险管理首先，我们需要考虑系统的可用性是什么。SRE 给出了如下两个公式：（1） 可用性 = 系统正常运行时间 / （系统正常运行时间+停机时间）（2） 可用性 = 成功请求数 / 总的请求数用户的需求以及SLO确定的可用性是很高的，但是并不是意味着我们不能一味地追求最高的可用性，我们还需要考虑成本和迭代速度。如下：可用性： 99.99% —-&gt; 99.99(…n个9)%成本： c1 —-&gt; c1*b^n ~ inf迭代速度： s1 —-&gt; s1/b^n ~ 0即：在可用性提高的同时，相应的系统成本就会显著提高、版本的迭代速度就会显著的下降。（这里的公式只是代表趋势，不能做实际的数值计算） 2 减少琐事何为琐事？ 琐事就是运维服务中手动性的，重复性的，可以被自动化的，战术性，没有持久价值的工作。而且，琐事与服务呈线性关系的增长。 手动性的：手动执行一些命令或者脚本。重复性的：需要不停地反复做的事情。可以被自动化的：如果计算机和人类一样可以完成某个任务。战术性的：突然出现的应对式的工作，而非计划内安排的。如处理紧急警报。没有持久价值：并非对服务带来永久性改进的工作。与服务同步线性增长：如果工作所涉及的任务与服务的大小、流量或用户数量呈线性增长关系，那这项任务可能就属于琐事。 为什么要减少琐事？ SRE 的一个公开的目标是保持每个 SRE 的工作时间中运维工作（即琐事）的比例低于 50% 。SRE 至少是 50% 的时间花在工程项目上，以减少未来的琐事或者增加服务功能。增加服务功能包括提高可靠性、性能、或利用率，同时也会进一步消除琐事。 什么是工程项目工作？工程项目工作是有创新性和创造性的，着重通过设计来解决问题。工程工作有助于团队在维持同等人员配备的情况下接手更大或者更多的服务。 典型的 SRE 活动分为如下几类：软件工程 —— 编写或者修改代码，以及所有其他相关的设计和文档工作。包括：项目特性代码，自动化脚本、工具代码等。系统工程 —— 配置、部署、更新等。琐事 —— 与运维相关的重复性的、手工的劳动。流程负担 —— 会议、总结、招聘、培训等。 3 自动化自动化的价值 操作的一致性，避免不同人不同时候做同一项配置的不同操作。 平台性，错误集中化，降低对人的依赖与要求（随时、频繁、精确）。 修复速度更快，有时候能够有效地降低 MTTR 。 节省时间，一个自动化系统可以被多次、多人执行。避免用人类的鲜血、汗水和眼泪来养活机器。 另外，通过一下这个图我们可以大致决定我们的工作是否需要被自动化。 图二 (From https://xkcd.com/1205/) 注：横轴表示任务执行的频率，纵轴表示每次任务执行时间，单元格表示 5 年的时间内，任务执行下来需要花费的时间。左下角的任务是花费时间最长的任务，是需要优先被自动化的任务。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[osd故障检测机制小结]]></title>
    <url>%2F2018%2F04%2F11%2Fosd%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E6%9C%BA%E5%88%B6%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[#osd故障检测机制小结从3条简要的代码线看osd的故障检测机制。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-heartbeat3]]></title>
    <url>%2F2017%2F12%2F20%2Fceph-heartbeat3%2F</url>
    <content type="text"><![CDATA[#ceph集群中的心跳机制研究3之前研究了Ceph集群的心跳机制和故障检测机制，那从心跳机制到故障判断的过程又是什么样的？因为涉及多个源代码文件，并且函数调用嵌套很多层，这里看起来不是那么清晰，所以这也是我最后要讲的地方。 ##从heartbeat check no reply到最终确定osd failure的过程heartbeat的相关实现在源文件OSD.cc中。1234567OSD::heartbeat() (send heartbeats) -&gt; OSD::heartbeat_check()...(省略一些中间过程)OSD::send_failures() -&gt; 依据failure_queue计算failed_for -&gt; 带上failed_for，通过MOSDFailure包装(带上标志MSG_OSD_FAILURE)，然后发送msg给mon，failed_for是OSD failed的时间长度。如下：monc-&gt;send_mon_message(new MOSDFailure(monc-&gt;get_fsid(), i, failed_for, osdmap-&gt;get_epoch())); 然后在源文件OSDMonitor.cc中进行处理。 123456789101112131415OSDMonitor::prepare_update() -&gt; case MSG_OSD_FAILURE ，它接受一个op参数，然后进入switch，如下：bool OSDMonitor::prepare_update(MonOpRequestRef op)&#123; op-&gt;mark_osdmon_event(__func__); PaxosServiceMessage *m = static_cast&lt;PaxosServiceMessage*&gt;(op-&gt;get_req()); dout(7) &lt;&lt; "prepare_update " &lt;&lt; *m &lt;&lt; " from " &lt;&lt; m-&gt;get_orig_source_inst() &lt;&lt; dendl; switch (m-&gt;get_type()) &#123; // damp updates case MSG_OSD_MARK_ME_DOWN: return prepare_mark_me_down(op); case MSG_OSD_FAILURE: return prepare_failure(op);...&#125; 最后在进入以下故障判定的两个阶段： 12OSDMonitor::prepare_failure()OSDMinitor::check_failure() 2017年12月20日]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-heartbeat2]]></title>
    <url>%2F2017%2F11%2F24%2Fceph-heartbeat2%2F</url>
    <content type="text"><![CDATA[#ceph集群中的心跳机制研究2 我知道Ceph的基石是RADOS，而RADOS的含义是Reliable Autonomic Distributed Object Storage，而Autonomic（自治）的实现则依赖于Ceph中很好的故障检测机制。下面进入正题。 Ceph在故障检测的时候有两个函数：12OSDMonitor::check_failureOSDMonitor::prepare_failure 他们之间的关系是，prepare_failure做了一些fail前的report过程，然后再调用check_failure。 ##grace time对于osd的故障判断有个grace time，相当于一个容错值，当fail time大于grace time时基本可以断定osd fail。当配置项g_conf-&gt;mon_osd_adjust_heartbeat_grace为true的时候，grace的计算有个过程，并且这个过程让人难以理解，先贴一下代码。 12345678910111213141516171819202122232425//my_grace计算 double halflife = (double)g_conf-&gt;mon_osd_laggy_halflife; double decay_k = ::log(.5) / halflife; // scale grace period based on historical probability of 'lagginess' // (false positive failures due to slowness). const osd_xinfo_t&amp; xi = osdmap.get_xinfo(target_osd); double decay = exp((double)failed_for * decay_k); dout(20) &lt;&lt; " halflife " &lt;&lt; halflife &lt;&lt; " decay_k " &lt;&lt; decay_k &lt;&lt; " failed_for " &lt;&lt; failed_for &lt;&lt; " decay " &lt;&lt; decay &lt;&lt; dendl; my_grace = decay * (double)xi.laggy_interval * xi.laggy_probability; grace += my_grace;//peer_grace计算 assert(fi.reporters.size()); for (map&lt;int,failure_reporter_t&gt;::iterator p = fi.reporters.begin(); p != fi.reporters.end(); ++p) &#123; const osd_xinfo_t&amp; xi = osdmap.get_xinfo(p-&gt;first); utime_t elapsed = now - xi.down_stamp; double decay = exp((double)elapsed * decay_k); peer_grace += decay * (double)xi.laggy_interval * xi.laggy_probability; &#125; peer_grace /= (double)fi.reporters.size(); grace += peer_grace; ##失效条件重点在这里，最终check_failure的成功条件是：123failed_for &gt;= grace &amp;&amp; ((int)fi.reporters.size() &gt;= g_conf-&gt;mon_osd_min_down_reporters) &amp;&amp;(fi.num_reports &gt;= g_conf-&gt;mon_osd_min_down_reports) 即失效时间大于grace(g_conf中设置的值)，并且reporters大于配置的reporters，reports大于配置的reports。当这3个条件同时满足后osd会被标记为down,并输出类似如下这样的日志：osd.3 failed 3 reports from 1 peers after 21.666000 &gt;= grace 20 ###注(2018年1月13日)：以上对于Ceph 0.94.5版本，对于Ceph10.2.10 Jewel版本，mark down的机制有所不同，条件从3个变成两个了，代码如下：12if (failed_for &gt;= grace &amp;&amp; (int)reporters_by_subtree.size() &gt;= g_conf-&gt;mon_osd_min_down_reporters) 即grace time超时，并且来自不同subtree的reporters达到了配置的数目。相关的两个集群配置为：12mon_osd_min_down_reporters，默认2mon_osd_reporter_subtree_level，默认为host ##grace值的tradeoff这个值可以在集群动态配置，默认是20s。如果值设置地过大，那么当集群出现节点故障时，集群的故障判断时间势必会很长，这直接导致客户端的IO相应地阻塞很长时间。如果值设置的过小也是可能会出问题的，因为分布式集群不可避免地会遇到节点之间的网络问题，当网络出现短时间的丢包或者阻塞的时候，过小的grace设置会导致集群很快认为osd down，尽管网络只是暂时的抖动。从这里可以看出来，对于分布式系统的3个状态：成功，失败和超时，其中失败和超时之间并没有严格的界限，需要我们自己去权衡。 2017年11月24日]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph heartbeat]]></title>
    <url>%2F2017%2F09%2F06%2Fceph-heartbeat%2F</url>
    <content type="text"><![CDATA[ceph集群中的心跳机制研究基于Ceph 0.94.5版本，进行Ceph集群中的心跳机制进行研究。在ceph中，心跳机制通过ping来实现，用来作为集群中故障检测的方法。分为两类：osd与osd之间的心跳，osd与mon之间的心跳。下面去一探究竟。 osd与osd之间的心跳1、相邻osd之间会维持心跳，默认情况下，时间间隔是6s。2、一个osd上面的pg所关联的osd之间存在着心跳。所谓相邻，即根据osd的ID，该osd前一个活着的osd和后一个活着的osd。 如果一个osd在20s的grace时间内，没有收到来自邻居osd的heartbeat，那么它认为这个邻居osd down，并且汇报给monitor。如果来自不同主机的2个osd报告同一个osd down，那么monitor承认这个osd down。 如下，从update_heartbeat_peers这个函数可以看出第2个选peers的方式。即有效的acting_set和up_set中出现的osd将被加入peers，但是涉及到 map pg_shard_t,pg_info_t &gt; 的加入方式并没有看懂，进一步研究。12345678910111213141516171819202122232425262728293031323334void PG::update_heartbeat_peers()&#123; assert(is_locked()); set&lt;int&gt; new_peers; if (is_primary()) &#123; for (unsigned i=0; i&lt;acting.size(); i++) &#123; if (acting[i] != CRUSH_ITEM_NONE) new_peers.insert(acting[i]); &#125; for (unsigned i=0; i&lt;up.size(); i++) &#123; if (up[i] != CRUSH_ITEM_NONE) new_peers.insert(up[i]); &#125; for (map&lt;pg_shard_t,pg_info_t&gt;::iterator p = peer_info.begin(); p != peer_info.end(); ++p) new_peers.insert(p-&gt;first.osd); &#125; bool need_update = false; heartbeat_peer_lock.Lock(); if (new_peers == heartbeat_peers) &#123; dout(10) &lt;&lt; "update_heartbeat_peers " &lt;&lt; heartbeat_peers &lt;&lt; " unchanged" &lt;&lt; dendl; &#125; else &#123; dout(10) &lt;&lt; "update_heartbeat_peers " &lt;&lt; heartbeat_peers &lt;&lt; " -&gt; " &lt;&lt; new_peers &lt;&lt; dendl; heartbeat_peers.swap(new_peers); need_update = true; &#125; heartbeat_peer_lock.Unlock(); if (need_update) osd-&gt;need_heartbeat_peer_update();&#125; osd与mon之间的心跳如果一个osd无法peer其他所有的osd，osd每30s ping一次monitor，并且获取最新的cluster map信息。如果osd没有主动报告monitor，那么monitor会在mon_osd_report_time时间后认为osd down。对应2个时间mon_osd_report_interval_min, mon_osd_report_interval_max。在达到interval_max的时候，不管有没有改变，osd都会主动上报给monitor。 osd在如下几种情况下会主动上报monitor：有失效的情况，pg状态的改变，up_thru的改变，osd在boot的5s时间以内。如果osd发现他的peers为空的时候，也会主动发送hb给monitor，并且获取新的osdmap（osdmap_subscribe）。 1234567891011struct HeartbeatInfo &#123; int peer; ///&lt; peer ConnectionRef con_front; ///&lt; peer connection (front) ConnectionRef con_back; ///&lt; peer connection (back) utime_t first_tx; ///&lt; time we sent our first ping request utime_t last_tx; ///&lt; last time we sent a ping request utime_t last_rx_front; ///&lt; last time we got a ping reply on the front side utime_t last_rx_back; ///&lt; last time we got a ping reply on the back side epoch_t epoch; ///&lt; most recent epoch we wanted this peer ......&#125; HeartbeatInfo结构体中包括：con_front表示前向的连接，即public network的连接，con_back表示后向连接，即cluster network的连接。当一个osd发送ping心跳报文的时候，会通过后向连接发送，如果前向连接有效，也通过前向连接发送，就是说正常情况下会同时发2个ping。12345678910i-&gt;second.con_back-&gt;send_message(new MOSDPing(monc-&gt;get_fsid(), service.get_osdmap()-&gt;get_epoch(), MOSDPing::PING, now)); if (i-&gt;second.con_front) i-&gt;second.con_front-&gt;send_message(new MOSDPing(monc-&gt;get_fsid(), service.get_osdmap()-&gt;get_epoch(), MOSDPing::PING, now)); 未完待续~ ~ ~2017年9月11日01:32:24]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storage]]></title>
    <url>%2F2017%2F09%2F03%2Fstorage%2F</url>
    <content type="text"><![CDATA[#A mind map about storage我个人现在的工作方向是云存储，这要求我既对目前接触的分布式存储方案、技术（Ceph）有深入的研究，对于存储这个较大的领域的一些基本原理、通用性、本质性的东西也很需要有所了解。先贴一张自己总结的思维导图，以后会基于此图进行补充或者细化。]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage, mind map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About hash function]]></title>
    <url>%2F2017%2F08%2F27%2Fhash-function%2F</url>
    <content type="text"><![CDATA[日常工作中经常会遇到哈希算法，出于好奇想多了解一些关于哈希的理论。在Ceph中（一种分布式存储系统），从Object到PG的映射就是采用了一种哈希算法，名为“rjenkins”。所以就是从这里作为切入点，看了一些资料。 哈希算法的作用哈希，也称作散列，它的作用其实就是通过一个函数（hash function），将输入值映射为一个输出的值，并且尽量保证这个映射是一对一的。 哈希算法的好坏怎样的哈希算法才是一个好的哈希算法？从他的作用或者说目的上看，它最重要的衡量标准是是否有碰撞（collision），或者是否有尽可能少的碰撞。这里有个概念 funneling，翻译成中文是“漏斗效应”[注1]，其实就是很多input被映射到少量的几个output上，产生了大量的碰撞。好的hash算法应该是可以通过测试或者理论证明这个算法是no funnel的。这里有两个概念：完美哈希函数（PHF，Perfect Hash Function）和最小完美哈希函数（MPHF，Minimal Perfect Hash Function）。完美哈希函数，就是没有冲突的哈希函数，也就是将N个KEY值映射到M个整数上，这里M&gt;=N，并且对于任意的KEY1，KEY2，H(KEY1)！=H(KEY2)。当M==N，则H是最小完美哈希函数。当然它的另一个衡量标准是速度快，前提是在相同硬件条件下。也就是说每次哈希的运算次数少或者消耗的CPU指令周期少。 哈希模型12345678initialize the internal state; for (each block of the input) &#123; combine (the internal state, the current input block); mix( the internal state); &#125; value = postprocess( the internal state ); return (value); 也就是说一个hash过程主要包括：initialize, combine, mix, postprocess这几个过程。 参考 Jenkins关于他新的hash算法以及相关理论的介绍 http://burtleburtle.net/bob/hash/evahash.html 几种hash算法的对比 http://burtleburtle.net/bob/hash/examhash.html 附言：好长时间没写东西了，今天来这里扫扫灰：）]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>hash, algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[string类型使用小结]]></title>
    <url>%2F2015%2F12%2F14%2Fstring%E7%B1%BB%E5%9E%8B%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[$$ string 类型使用小结 $$ c++在c的char类型基础上引入了string类，从而更加方便地对字符串进行操作。但是如果是不会使用string类也会带来很多麻烦。在自己编码的基础上总结了几点误区。 ##1、char*类型与string类型的相互转换从char*到string，需要调用string类的构造函数 如：char* a; string s(a);从string到char*，需要使用string类的c_str()函数 如：string s = “hello”, const char* c = s.c_str(); ##2、一个特殊错误 单步调试没问题，但是一运行就崩溃 1234567891011//编译通过，运行崩溃 string str; i=0; freopen("test_without_punctuation.txt", "r", stdin); char c; while( scanf("%c", &amp;str[i]) != EOF) &#123; if(str[i] &gt;= 'a' &amp;&amp; str[i] &lt;= 'z') total_len += MC[str[i]-'a'].length(); // cout &lt;&lt; total_len &lt;&lt; ' ' &lt;&lt; i &lt;&lt; endl; ++i; &#125; 123456789101112//编译通过，运行正确！ string str; i=0; freopen("test_without_punctuation.txt", "r", stdin); char c; while( scanf("%c", &amp;c) != EOF) &#123; str+=c; if(str[i] &gt;= 'a' &amp;&amp; str[i] &lt;= 'z') total_len += MC[str[i]-'a'].length(); // cout &lt;&lt; total_len &lt;&lt; ' ' &lt;&lt; i &lt;&lt; endl; ++i; &#125; 原因可能在于对string类型进行读入的时候，即使用scanf()时，string不支持对单个字符的读入！]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>string类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Huffman编码与Morse编码]]></title>
    <url>%2F2015%2F12%2F14%2FHuffman%E7%BC%96%E7%A0%81%E4%B8%8EMorse%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[最近做课程设计，接触到了Huffman编码与Morse编码。在此总结，分享。之前认为编码是个很神奇的存在，现在能够用代码实现，感觉还是很开心的！ #一、Huffman编码 ##1、概念（摘自wikipedia） 霍夫曼编码（Huffman Coding）是一种编码方式，是一种用于无损数据压缩的熵编码（权编码）演算法。也称“哈夫曼编码”，“赫夫曼编码”。1952年，David A. Huffman在麻省理工攻读博士时所发明的，并发表于《一种构建极小多馀编码的方法》（A Method for the Construction of Minimum-Redundancy Codes）一文。 在计算机资料处理中，霍夫曼编码使用变长编码表对源符号（如文件中的一个字母）进行编码，其中变长编码表是通过一种评估来源符号出现机率的方法得到的，出现机率高的字母使用较短的编码，反之出现机率低的则使用较长的编码，这便使编码之后的字符串的平均长度、期望值降低，从而达到无损压缩数据的目的。 例如，在英文中，e的出现机率最高，而z的出现概率则最低。当利用霍夫曼编码对一篇英文进行压缩时，e极有可能用一个位元来表示，而z则可能花去25个位元（不是26）。用普通的表示方法时，每个英文字母均占用一个字节，即8个位元。二者相比，e使用了一般编码的1/8的长度，z则使用了3倍多。倘若我们能实现对于英文中各个字母出现概率的较准确的估算，就可以大幅度提高无损压缩的比例。 霍夫曼树又称最优二叉树，是一种带权路径长度最短的二叉树。所谓树的带权路径长度，就是树中所有的叶结点的权值乘上其到根结点的路径长度（若根结点为0层，叶结点到根结点的路径长度为叶结点的层数）。树的路径长度是从树根到每一结点的路径长度之和，记为WPL=（W1L1+W2L2+W3L3+…+WnLn），N个权值Wi（i=1,2,…n）构成一棵有N个叶结点的二叉树，相应的叶结点的路径长度为Li（i=1,2,…n）。可以证明霍夫曼树的WPL是最小的。 ##2、算法构造huffman树的哈夫曼算法如下： （1）n节点的权值｛w1、w2、·····，wn｝构成n棵二叉树集合F=｛T1，T2，···，Tn｝，每棵二叉树Ti只有一个带权为Wi的根节点，左右孩子均空。 （2）在F中选取两棵根节点权值最小的作为树的左右孩子构造一棵新的二叉树，且置根节点的权值为左右孩子权值之和，在F中删除这两棵树，新二叉树放于集合F中。 （3）重复（2），直到F中只有一棵树为止，这棵树就是huffman树。 ##3、实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/* 说明： 1、此程序用来构建Huffman树，对英文字母a-z根据词频统计生成Huffman码表； 2、对指定文段进行Huffman编码，并计算编码总长度和平均码长； 3、IDE为Dev C++ 5.4.1*/#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;using namespace std;//构造树节点，parent作为判断这棵树是否在集合中，//lchild为权值最小节点，rchild为权值第二小的节点，weight为节点的权值。typedef struct &#123; int weight; int parent, lchild, rchild;&#125;HuffmanNode, *HuffmanTree;typedef char **HuffmanCode;void HuffmanCoding(HuffmanTree &amp;HT, HuffmanCode &amp;HC, float *w, int n);void HuffmanDecoding(HuffmanCode &amp;HC, char* code_str, char* decode_str);void select(HuffmanTree HT, int n, int &amp;s1, int &amp;s2);//全局变量 float w[26] = &#123;8.167, 1.492, 2.782, 4.253, 12.702, 2.228, 2.015, 6.094, 6.966, 0.153, 0.772, 4.025, 2.406, 6.749, 7.507, 1.929, 0.095, 5.987, 6.327, 9.056, 2.758, 0.978, 2.361, 0.150, 1.974, 0.074&#125;;HuffmanTree HT; HuffmanCode HC;//主函数 //-----------------------------------------------------int main (void) &#123; // string code_str; string str; int i=0; double total_len=0.0, average_len=0.0; HuffmanCoding(HT, HC, w, 26); freopen("test_without_punctuation.txt", "r", stdin); char c; while( scanf("%c", &amp;c) != EOF) &#123; str+=c; if(str[i] &gt;= 'a' &amp;&amp; str[i] &lt;= 'z') total_len += strlen(HC[str[i]-'a'+1]); // cout &lt;&lt; total_len &lt;&lt; ' ' &lt;&lt; i &lt;&lt; endl; ++i; &#125; //输出 cout &lt;&lt; "Total length of HuffmanCoding is: " &lt;&lt; total_len &lt;&lt; endl; average_len = total_len / i; cout &lt;&lt; "Average length of HuffmanCoding is: " &lt;&lt; average_len &lt;&lt; endl; free(HC); free(HT); return 0;&#125;//-------------------------------------------------------//Huffman编码函数 void HuffmanCoding(HuffmanTree &amp;HT, HuffmanCode &amp;HC, float *w, int n) &#123; int m = 2*n-1; int s1, s2; HT = (HuffmanNode*)malloc((m+1)*sizeof(HuffmanNode)); memset(HT, 0, (m+1)*sizeof(HuffmanNode)); //初始化权值 for(int i=1; i&lt;=n; i++) HT[i].weight = *w++; //创建Huffman tree for(int i=n+1; i&lt;=m; i++) &#123; //选择剩余节点中权值最小的节点s1,s2 select(HT, i-1, s1, s2); HT[s1].parent = i; HT[s2].parent = i; HT[i].lchild = s1; HT[i].rchild = s2; HT[i].weight = HT[s1].weight + HT[s2].weight; &#125; int start, c, f; HC = (HuffmanCode)malloc((n+1)*sizeof(char*)); char* cd = (char*)malloc(n*sizeof(char)); cd[n-1] = '\0'; for(int i=1; i&lt;=n; i++) &#123; start = n-1; for(c=i, f=HT[i].parent; f!=0; c=f,f=HT[f].parent) if(HT[f].lchild == c) cd[--start] = '0'; else cd[--start] = '1'; HC[i] = (char*)malloc((n-start)*sizeof(char)); strcpy(HC[i], &amp;cd[start]); &#125; //输出a-z的Huffman编码 cout &lt;&lt; "The HuffmanCode of a to z:\n"; for(int i=1; i&lt;=n; i++) cout &lt;&lt; (char)('a'+i-1) &lt;&lt; " : " &lt;&lt; HC[i] &lt;&lt; endl; free(cd);&#125; //最小权值选择函数 void select(HuffmanTree HT, int n, int &amp;s1, int &amp;s2) &#123; s1 = 1; s2 = 1; int min = 9999; int i; //找s1 for(int p = 1; p&lt;=n; p++) &#123; if(0 == HT[p].parent &amp;&amp; min &gt;= HT[p].weight) &#123; s1 = p; min = HT[p].weight; &#125; &#125; //找s2 min = 9999; for(int q = 1; q&lt;=n; q++) &#123; if(0 == HT[q].parent &amp;&amp; min &gt;= HT[q].weight) &#123; if(q == s1) continue; s2 = q; min = HT[q].weight; &#125; &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839//Huffman译码函数,仅供参考，有BUGvoid HuffmanDecoding(HuffmanCode &amp;HC, char* code_str, char* decode_str) &#123; // char decode_str[30]; int j=0,k=0; //j为a-z序号，k为每段译码的序号 int t=0,s=0; //t变量作为检查是否编码被排除，s作为解码之后的字符数组序号 int cnt=0; bool is_code[26]; int test[26]; memset(test, 0, sizeof(test)); memset(is_code, true, sizeof(is_code)); int len = strlen(code_str); for(int i=0; i&lt;len; i++) &#123; for(j=1; j&lt;=26; j++) if(k&gt;=strlen(HC[j]) || code_str[i] != HC[j][k]) is_code[j-1] = false; k++; cnt=0; memset(test, 0, sizeof(test)); for(t=0; t&lt;26; t++) if(is_code[t]) &#123; cnt++; test[cnt] = t; &#125; if(cnt == 1 || (cnt == 2 &amp;&amp; i == 15)) &#123; for(t=0; t&lt;26; t++) if(is_code[t]) &#123; if(i == 15) decode_str[s++] = 'a'+7; else decode_str[s++] = 'a'+t; break; &#125; k=0; memset(is_code, true, sizeof(is_code)); &#125; &#125; cout &lt;&lt; decode_str &lt;&lt; endl;&#125; #二、Morse编码 ##1、概念(摘自wikipedia) 摩尔斯电码（英语：Morse Code）是一种时通时断的信号代码，通过不同的排列顺序来表达不同的英文字母、数字和标点符号。是由美国人萨缪尔·摩尔斯在1836年发明。 摩尔斯电码是一种早期的数码化通信形式，但是它不同于现代只使用0和1两种状态的二进制代码，它的代码包括五种： 1.点（.） 2.划（-） 3.每个字符间短的停顿（在点和划之间的停顿） 4.每个词之间中等的停顿 5.以及句子之间长的停顿 A-Z的Morse编码如下: ##2、算法即将每个英文字母转化为对应的Morse码，在一个字母的内部、字母之间、单词之间都需要添加特定个数的空格。 ##3、实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* 说明： 1、根据英文字母a-z的Morse码表，对指定文段进行Morse编码，并计算编码总长度和平均码长； 3、IDE为Dev C++ 5.4.1*/#include &lt;iostream&gt;#include &lt;string.h&gt;#include &lt;fstream&gt;#include &lt;stdlib.h&gt;using namespace std;int main (void) &#123; string str; string code_str; string MC[26]; double total_len=0.0; double average_len=0.0; int i=0; //按行读取文件 ifstream fin( "Morsecode.txt" ); while ( getline(fin, MC[i]) ) i++; fin.close(); //输出morse码表 cout &lt;&lt; "The MorseCode of a to z:\n"; for(i=0; i&lt;26; i++) cout &lt;&lt; (char)('a'+i) &lt;&lt; " : " &lt;&lt; MC[i] &lt;&lt; endl; //输出重定向并统计文段morse码总长度 i=0; freopen("test_without_punctuation.txt", "r", stdin); char c; while( scanf("%c", &amp;c) != EOF) &#123; str+=c; if(str[i] &gt;= 'a' &amp;&amp; str[i] &lt;= 'z') total_len += MC[str[i]-'a'].length(); // cout &lt;&lt; total_len &lt;&lt; ' ' &lt;&lt; i &lt;&lt; endl; ++i; &#125; //输出 cout &lt;&lt; "Total length of MorseCoding is: " &lt;&lt; total_len &lt;&lt; endl; average_len = total_len / i; cout &lt;&lt; "Average length of MorseCoding is: " &lt;&lt; average_len &lt;&lt; endl; return 0;&#125; Reference:[1].https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81[2].http://blog.csdn.net/qyee16/article/details/6664377[3].https://zh.wikipedia.org/wiki/%E6%91%A9%E5%B0%94%E6%96%AF%E7%94%B5%E7%A0%81]]></content>
      <categories>
        <category>编码</category>
      </categories>
      <tags>
        <tag>编码、Huffman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[strcpy与memcpy的实现]]></title>
    <url>%2F2015%2F11%2F17%2Fstrcpy%E4%B8%8Ememcpy%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[因为之前面试的时候遇见这样的题，今天又遇到了，才想起来，所以拿出来总结一下。 12345678//strcpy不合格实现char *my_strcpy(char* dst, const char* src) &#123; assert(dst != NULL); assert(src != NULL); char* ret = dst; while((*dst++ = *src++) != '\0') ; return ret;&#125; 考虑特殊情况： 12char a[] = "abc";my_strcpy(a+1, a); 这时程序会崩溃！！ 12345678//strcpy正确实现（考虑到内存重叠问题）char *my_strcpy(char* dst, const char* src) &#123; assert(dst != NULL); assert(src != NULL); char* ret = dst; memcpy(dst, src, strlen(src)+1); return ret;&#125; 同理，memcpy函数的实现也需要考虑内存重叠问题。实现如下： 123456789101112131415161718192021222324252627282930//memcpy的实现void * my_memcpy(void *dst,const void *src,unsigned int count)&#123; assert(dst); assert(src); void * ret = dst; //源地址和目的地址不重叠，低字节向高字节拷贝 if (dst &lt;= src || (char *)dst &gt;= ((char *)src + count)) &#123; while(count--) &#123; *(char *)dst = *(char *)src; dst = (char *)dst + 1; src = (char *)src + 1; &#125; &#125; //源地址和目的地址重叠，高字节向低字节拷贝 else &#123; dst = (char *)dst + count - 1; src = (char *)src + count - 1; while(count--) &#123; *(char *)dst = *(char *)src; dst = (char *)dst - 1; src = (char *)src - 1; &#125; &#125; return ret;&#125; 调用char *strcpy(char *dst, char *src)函数时，当dst = NULL 或者 src = NULL时，程序崩溃，正是由于assert()函数使程序终止。 但是深入一点就有疑惑了，为什么内存拷贝当src&lt;dst&lt;src+count 时会出错呢？为什么要单独考虑这种情况呢？欢迎交流！ 即便是strcpy、memcpy这样的c函数也还是有一些讲究的！学无止境！]]></content>
      <categories>
        <category>c</category>
      </categories>
      <tags>
        <tag>c</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux网络连接问题解决]]></title>
    <url>%2F2015%2F11%2F05%2Flinux%E6%96%AD%E7%BD%91%E6%80%A5%E6%95%91%2F</url>
    <content type="text"><![CDATA[#Linux网络连接问题解决 因为linux网络编程实验需要，需要对linux虚拟机进行联网。但是其中遇到了各种问题，尝试了很多解决方法。拿出来总结、分享一下。 1、基本指令与配置： ifconfig 查看网卡信息route -n 显示现在所有路由ping IP地址（或者域名） 测试网络连接状况sudo /etc/init.d/network restart 重启网络 网关配置：增加网关前提是网关出现在route -n 的结果中，否则报错，进程不存在sudo route add default gw 169.254.0.1 dns服务器配置/etc/resolv.conf dns生效优先级配置dns 与 files（指/etc/hosts）顺序，让dns在前面的优先生效grep hosts /etc/nsswitch.conf/etc/nsswitch.conf/etc/hosts 2、配置文件 /etc/network/interfaces 默认的环回地址auto loiface lo inet loopback静态配置auto eth0iface eth0 inet staticaddress 192.168.2.120（例子）gateway 192.168.2.1netmask 255.255.255.0动态配置auto eth0iface eth0 inet dhcp 3、网络不通的基本排错步骤（从上往下） ping 127.0.0.1 ping的通说明tcp协议栈没有问题ping 主机地址 ping的通说明网卡没有问题ping 路由器默认网关 ping的通说明包可以到达路由器ping DNS服务器地址卡在哪一步，就补哪里 4、说明 本人对这方面也是新手，发现问题多多交流，欢迎在本人博客或者微博留言。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
